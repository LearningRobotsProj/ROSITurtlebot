# PPO Algorithm Parameters
algorithm: "ppo"
policy: "MlpPolicy"
normalize_observations: true
normalize_advantages: true

# Network Parameters
net_arch:
  pi: [256, 128, 64]
  vf: [256, 128, 64]
activation_fn: "relu"

# Optimization
learning_rate_schedule: "constant"
clip_range_schedule: "constant"
ent_coef_schedule: "constant"

# Environment
n_envs: 1
n_steps: 2048